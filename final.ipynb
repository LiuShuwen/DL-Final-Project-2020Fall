{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", 101)\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"font.size\"] = 15\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = './data/'\n",
    "\n",
    "# data = pd.read_csv(path + 'train.csv')\n",
    "# data['ClassId'] = data['ClassId'].astype(int)\n",
    "# data = data.pivot(index='ImageId',columns='ClassId',values='EncodedPixels')\n",
    "\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_dict = defaultdict(int)\n",
    "\n",
    "# kind_class_dict = defaultdict(int)\n",
    "\n",
    "# no_defects_num = 0\n",
    "# defects_num = 0\n",
    "\n",
    "# for i in range(0, len(data)):\n",
    "#     img_names = [data.iloc[i, 0]]\n",
    "        \n",
    "#     labels = data.iloc[i, 0:4]\n",
    "\n",
    "#     if labels.isna().all():\n",
    "#         no_defects_num += 1\n",
    "#     else:\n",
    "#         defects_num += 1\n",
    "    \n",
    "#     kind_class_dict[sum(labels.isna().values == False)] += 1\n",
    "        \n",
    "#     for idx, label in enumerate(labels.isna().values.tolist()):\n",
    "#         if label == False:\n",
    "#             class_dict[idx+1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"the number of images with no defects: {}\".format(no_defects_num))\n",
    "# print(\"the number of images with defects: {}\".format(defects_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# sns.barplot(x=list(class_dict.keys()), y=list(class_dict.values()), ax=ax)\n",
    "# ax.set_title(\"the number of images for each class\")\n",
    "# ax.set_xlabel(\"class\")\n",
    "# class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# sns.barplot(x=list(kind_class_dict.keys()), y=list(kind_class_dict.values()), ax=ax)\n",
    "# ax.set_title(\"Number of classes included in each image\");\n",
    "# ax.set_xlabel(\"number of classes in the image\")\n",
    "# kind_class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size_dict = defaultdict(int)\n",
    "# train_path = Path(\"./data/train_images/\")\n",
    "\n",
    "# for img_name in train_path.iterdir():\n",
    "#     img = Image.open(img_name)\n",
    "#     train_size_dict[img.size] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_size_dict = defaultdict(int)\n",
    "# test_path = Path(\"./data/test_images/\")\n",
    "\n",
    "# for img_name in test_path.iterdir():\n",
    "#     img = Image.open(img_name)\n",
    "#     test_size_dict[img.size] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = './data/'\n",
    "\n",
    "# data = pd.read_csv(path + 'train.csv')\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[data['EncodedPixels'].notnull()].reset_index(drop=True)\n",
    "# print(len(data))\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask(row_id, df):\n",
    "    '''Given a row index, return image_id and mask (256, 1600, 4) from the dataframe `df`'''\n",
    "    fname = df.iloc[row_id].name\n",
    "    labels = df.iloc[row_id][:4]\n",
    "    masks = np.zeros((256, 1600, 4), dtype=np.float32)\n",
    "\n",
    "    for idx, label in enumerate(labels.values):\n",
    "        if label is not np.nan:\n",
    "            label = label.split(\" \")\n",
    "            positions = map(int, label[0::2])\n",
    "            length = map(int, label[1::2])\n",
    "            mask = np.zeros(256 * 1600, dtype=np.uint8)\n",
    "            for pos, le in zip(positions, length):\n",
    "                mask[pos:(pos + le)] = 1\n",
    "            masks[:, :, idx] = mask.reshape(256, 1600, order='F')\n",
    "    return fname, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import HorizontalFlip, Normalize, Resize, Compose\n",
    "from albumentations.pytorch import ToTensor\n",
    "\n",
    "def get_transforms(phase):\n",
    "    list_transforms = []\n",
    "#     if phase == \"train\":\n",
    "#         list_transforms.extend(\n",
    "#             [\n",
    "#                 HorizontalFlip(p=0.5), # only horizontal flip as of now\n",
    "#             ]\n",
    "#         )\n",
    "    list_transforms.extend(\n",
    "        [\n",
    "            Resize(256, 256),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], p=1),\n",
    "            ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    list_trfms = Compose(list_transforms)\n",
    "    return list_trfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SteelDataset(Dataset):\n",
    "    def __init__(self, df, data_folder, phase):\n",
    "        self.df = df\n",
    "        self.root = data_folder\n",
    "        self.phase = phase\n",
    "        self.transforms = get_transforms(phase)\n",
    "        self.fnames = self.df.index.tolist()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id, mask = make_mask(idx, self.df)\n",
    "        image_path = os.path.join(self.root, \"train_images\",  image_id)\n",
    "        img = cv2.imread(image_path)\n",
    "        augmented = self.transforms(image=img, mask=mask)\n",
    "        img = augmented['image']\n",
    "        mask = augmented['mask'] # 1x256x1600x4\n",
    "        mask = mask[0].permute(2, 0, 1) # 4x256x1600\n",
    "        return img, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def SteelDataLoader(\n",
    "    data_folder,\n",
    "    df,\n",
    "    phase,\n",
    "    batch_size=8,\n",
    "    num_workers=4,\n",
    "):\n",
    "    '''Returns dataloader for the model training'''\n",
    "    image_dataset = SteelDataset(df, data_folder, phase)\n",
    "    dataloader = DataLoader(\n",
    "        image_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,   \n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "# data['ClassId'] = data['ClassId'].astype(int)\n",
    "# data = data.pivot(index='ImageId',columns='ClassId',values='EncodedPixels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, val_df = train_test_split(data, test_size=0.2, stratify=data.count(axis=1), random_state=69)\n",
    "# train_loader = SteelDataLoader(\"./data/\", train_df, \"train\", 4, 4)\n",
    "# val_loader = SteelDataLoader(\"./data/\", val_df, \"val\", 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/new_train.csv\", index_col=\"ImageId\")\n",
    "val_df = pd.read_csv(\"./data/new_test.csv\", index_col=\"ImageId\")\n",
    "train_loader = SteelDataLoader(\"./data/\", train_df, \"train\", 4, 4)\n",
    "val_loader = SteelDataLoader(\"./data/\", val_df, \"val\", 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ImageId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ca492d6f1.jpg</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154203 90 154370 179 154626 179 154882 179 155...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8b495e389.jpg</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>258819 10 259075 30 259331 49 259587 69 259843...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ebffb5d31.jpg</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14420 5 14676 14 14932 22 15187 28 15443 28 15...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ce22cd852.jpg</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40462 1 40717 3 40971 5 41226 7 41481 8 41736 ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24e3826ff.jpg</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>256509 4 256758 11 257006 19 257255 26 257507 ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 1    2                                                  3  \\\n",
       "ImageId                                                                      \n",
       "ca492d6f1.jpg  NaN  NaN  154203 90 154370 179 154626 179 154882 179 155...   \n",
       "8b495e389.jpg  NaN  NaN  258819 10 259075 30 259331 49 259587 69 259843...   \n",
       "ebffb5d31.jpg  NaN  NaN  14420 5 14676 14 14932 22 15187 28 15443 28 15...   \n",
       "ce22cd852.jpg  NaN  NaN  40462 1 40717 3 40971 5 41226 7 41481 8 41736 ...   \n",
       "24e3826ff.jpg  NaN  NaN  256509 4 256758 11 257006 19 257255 26 257507 ...   \n",
       "\n",
       "                 4  \n",
       "ImageId             \n",
       "ca492d6f1.jpg  NaN  \n",
       "8b495e389.jpg  NaN  \n",
       "ebffb5d31.jpg  NaN  \n",
       "ce22cd852.jpg  NaN  \n",
       "24e3826ff.jpg  NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5332"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1334"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, threshold):\n",
    "    '''X is sigmoid output of the model'''\n",
    "    X_p = np.copy(X)\n",
    "    preds = (X_p > threshold).astype('uint8')\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dice(probability, truth, threshold=0.5, reduction='none'):\n",
    "    '''Calculates dice of positive and negative images seperately'''\n",
    "    '''probability and truth must be torch tensors'''\n",
    "    batch_size = len(truth)\n",
    "    with torch.no_grad():\n",
    "        probability = probability.view(batch_size, -1)\n",
    "        truth = truth.view(batch_size, -1)\n",
    "        assert(probability.shape == truth.shape)\n",
    "\n",
    "        p = (probability > threshold).float()\n",
    "        t = (truth > 0.5).float()\n",
    "\n",
    "        t_sum = t.sum(-1)\n",
    "        p_sum = p.sum(-1)\n",
    "        neg_index = torch.nonzero(t_sum == 0, as_tuple=False)\n",
    "        pos_index = torch.nonzero(t_sum >= 1, as_tuple=False)\n",
    "\n",
    "        dice_neg = (p_sum == 0).float()\n",
    "        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1))\n",
    "\n",
    "        dice_neg = dice_neg[neg_index]\n",
    "        dice_pos = dice_pos[pos_index]\n",
    "        dice = torch.cat([dice_pos, dice_neg])\n",
    "\n",
    "        num_neg = len(neg_index)\n",
    "        num_pos = len(pos_index)\n",
    "\n",
    "    return dice, dice_neg, dice_pos, num_neg, num_pos\n",
    "\n",
    "\n",
    "def compute_ious(pred, label, classes, ignore_index=255, only_present=True):\n",
    "    pred[label == ignore_index] = 0\n",
    "    ious = []\n",
    "    for c in classes:\n",
    "        label_c = label == c\n",
    "        if only_present and np.sum(label_c) == 0:\n",
    "            ious.append(np.nan)\n",
    "            continue\n",
    "        pred_c = pred == c\n",
    "        intersection = np.logical_and(pred_c, label_c).sum()\n",
    "        union = np.logical_or(pred_c, label_c).sum()\n",
    "        if union != 0:\n",
    "            ious.append(intersection / union)\n",
    "    return ious if ious else [1]\n",
    "\n",
    "def compute_iou_batch(outputs, labels, classes=None):\n",
    "    ious = []\n",
    "    preds = np.copy(outputs)\n",
    "    labels = np.array(labels)\n",
    "    for pred, label in zip(preds, labels):\n",
    "        ious.append(np.nanmean(compute_ious(pred, label, classes)))\n",
    "    iou = np.nanmean(ious)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    '''A Logger to keep track of iou and dice scores throughout an epoch'''\n",
    "    def __init__(self, phase, epoch):\n",
    "        self.base_threshold = 0.5\n",
    "        self.base_dice_scores = []\n",
    "        self.dice_neg_scores = []\n",
    "        self.dice_pos_scores = []\n",
    "        self.iou_scores = []\n",
    "\n",
    "    def update(self, targets, outputs):\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        dice, dice_neg, dice_pos, _, _ = compute_dice(probs, targets, self.base_threshold)\n",
    "        self.base_dice_scores.extend(dice.tolist())\n",
    "        self.dice_pos_scores.extend(dice_pos.tolist())\n",
    "        self.dice_neg_scores.extend(dice_neg.tolist())\n",
    "        preds = predict(probs, self.base_threshold)\n",
    "        iou = compute_iou_batch(preds, targets, classes=[1])\n",
    "        self.iou_scores.append(iou)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        dice = np.nanmean(self.base_dice_scores)\n",
    "        dice_neg = np.nanmean(self.dice_neg_scores)\n",
    "        dice_pos = np.nanmean(self.dice_pos_scores)\n",
    "        dices = [dice, dice_neg, dice_pos]\n",
    "        iou = np.nanmean(self.iou_scores)\n",
    "        return dices, iou\n",
    "\n",
    "def epoch_log(epoch, epoch_loss, meter):\n",
    "    '''logging the metrics at the end of an epoch'''\n",
    "    dices, iou = meter.get_metrics()\n",
    "    dice, dice_neg, dice_pos = dices\n",
    "    print(\"Loss: %0.4f | IoU: %0.4f | dice: %0.4f | dice_neg: %0.4f | dice_pos: %0.4f\" % (epoch_loss, iou, dice, dice_neg, dice_pos))\n",
    "    return dice, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "model = smp.Unet(\"resnet18\", encoder_weights=\"imagenet\", classes=4, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# # define hyperparameters\n",
    "# num_epochs = 20\n",
    "# batch_size = 4\n",
    "# accumulation_steps = 32 / batch_size\n",
    "# best_loss = float(\"inf\")\n",
    "\n",
    "# train_losses = []\n",
    "# train_iou = []\n",
    "# train_dice = []\n",
    "# valid_losses = []\n",
    "# valid_iou = []\n",
    "# valid_dice = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training\n",
    "#     log = Logger(\"train\", epoch)\n",
    "#     start = time.strftime(\"%H:%M:%S\")\n",
    "#     print(f\"Starting epoch: {epoch} | phase: train | ⏰: {start}\")\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     total_batches = len(train_loader)\n",
    "#     optimizer.zero_grad()\n",
    "#     for itr, batch in enumerate(train_loader):\n",
    "#         images, targets = batch\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         loss = loss / accumulation_steps\n",
    "#         loss.backward()\n",
    "#         if (itr + 1 ) % accumulation_steps == 0:\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "#         running_loss += loss.item()\n",
    "#         outputs = outputs.detach()\n",
    "#         log.update(targets, outputs)\n",
    "#     epoch_loss = (running_loss * accumulation_steps) / total_batches\n",
    "#     dice, iou = epoch_log(epoch, epoch_loss, log)\n",
    "#     train_losses.append(epoch_loss)\n",
    "#     train_dice.append(dice)\n",
    "#     train_iou.append(iou)\n",
    "    \n",
    "#     state = {\n",
    "#         \"epoch\": epoch,\n",
    "#         \"best_loss\": best_loss,\n",
    "#         \"state_dict\": model.state_dict(),\n",
    "#         \"optimizer\": optimizer.state_dict(),\n",
    "#     }\n",
    "    \n",
    "#     # Validation\n",
    "#     torch.no_grad()\n",
    "#     val_log = Logger(\"val\", epoch)\n",
    "#     start = time.strftime(\"%H:%M:%S\")\n",
    "#     print(f\"Starting epoch: {epoch} | phase: val | ⏰: {start}\")\n",
    "#     running_loss = 0.0\n",
    "#     total_batches = len(val_loader)\n",
    "#     optimizer.zero_grad()\n",
    "#     for itr, batch in enumerate(val_loader):\n",
    "#         images, targets = batch\n",
    "#         outputs = model(images)\n",
    "#         loss = criterion(outputs, targets)\n",
    "#         loss = loss / accumulation_steps\n",
    "#         running_loss += loss.item()\n",
    "#         outputs = outputs.detach()\n",
    "#         log.update(targets, outputs)\n",
    "#     val_loss = (running_loss * accumulation_steps) / total_batches\n",
    "#     dice, iou = epoch_log(epoch, epoch_loss, log)\n",
    "#     valid_losses.append(val_loss)\n",
    "#     valid_dice.append(dice)\n",
    "#     valid_iou.append(iou)\n",
    "    \n",
    "#     scheduler.step(val_loss)\n",
    "    \n",
    "#     if val_loss < best_loss:\n",
    "#         print(\"******** New optimal found, saving state ********\")\n",
    "#         state[\"best_loss\"] = best_loss = val_loss\n",
    "#         torch.save(state, \"./model.pth\")\n",
    "#     print()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: train | ⏰: 16:56:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-930a3d73628a>:51: RuntimeWarning: Mean of empty slice\n",
      "  ious.append(np.nanmean(compute_ious(pred, label, classes)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0715 | IoU: 0.0466 | dice: 0.0696 | dice_neg: 1.0000 | dice_pos: 0.0695\n",
      "Starting epoch: 0 | phase: val | ⏰: 17:47:14\n",
      "Loss: 0.0715 | IoU: 0.0610 | dice: 0.0910 | dice_neg: 1.0000 | dice_pos: 0.0909\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 1 | phase: train | ⏰: 17:52:06\n",
      "Loss: 0.0453 | IoU: 0.1277 | dice: 0.1880 | dice_neg: 1.0000 | dice_pos: 0.1879\n",
      "Starting epoch: 1 | phase: val | ⏰: 18:42:23\n",
      "Loss: 0.0453 | IoU: 0.1362 | dice: 0.1999 | dice_neg: 1.0000 | dice_pos: 0.1998\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 2 | phase: train | ⏰: 18:47:18\n",
      "Loss: 0.0432 | IoU: 0.1594 | dice: 0.2295 | dice_neg: 1.0000 | dice_pos: 0.2294\n",
      "Starting epoch: 2 | phase: val | ⏰: 19:37:39\n",
      "Loss: 0.0432 | IoU: 0.1556 | dice: 0.2255 | dice_neg: 1.0000 | dice_pos: 0.2254\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 3 | phase: train | ⏰: 19:42:33\n",
      "Loss: 0.0409 | IoU: 0.1821 | dice: 0.2588 | dice_neg: 1.0000 | dice_pos: 0.2586\n",
      "Starting epoch: 3 | phase: val | ⏰: 20:32:55\n",
      "Loss: 0.0409 | IoU: 0.1794 | dice: 0.2556 | dice_neg: 1.0000 | dice_pos: 0.2555\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 4 | phase: train | ⏰: 20:37:49\n",
      "Loss: 0.0397 | IoU: 0.1831 | dice: 0.2607 | dice_neg: 1.0000 | dice_pos: 0.2606\n",
      "Starting epoch: 4 | phase: val | ⏰: 21:28:15\n",
      "Loss: 0.0397 | IoU: 0.1884 | dice: 0.2677 | dice_neg: 1.0000 | dice_pos: 0.2676\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 5 | phase: train | ⏰: 21:33:09\n",
      "Loss: 0.0380 | IoU: 0.2099 | dice: 0.2962 | dice_neg: 1.0000 | dice_pos: 0.2960\n",
      "Starting epoch: 5 | phase: val | ⏰: 22:23:39\n",
      "Loss: 0.0380 | IoU: 0.2044 | dice: 0.2885 | dice_neg: 1.0000 | dice_pos: 0.2884\n",
      "******** No improvement with patience: 1/4 ********\n",
      "\n",
      "Starting epoch: 6 | phase: train | ⏰: 22:28:32\n",
      "Loss: 0.0369 | IoU: 0.2224 | dice: 0.3119 | dice_neg: 1.0000 | dice_pos: 0.3118\n",
      "Starting epoch: 6 | phase: val | ⏰: 23:19:05\n",
      "Loss: 0.0369 | IoU: 0.2258 | dice: 0.3167 | dice_neg: 1.0000 | dice_pos: 0.3166\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 7 | phase: train | ⏰: 23:24:01\n",
      "Loss: 0.0356 | IoU: 0.2395 | dice: 0.3334 | dice_neg: 1.0000 | dice_pos: 0.3333\n",
      "Starting epoch: 7 | phase: val | ⏰: 00:14:41\n",
      "Loss: 0.0356 | IoU: 0.2424 | dice: 0.3369 | dice_neg: 1.0000 | dice_pos: 0.3368\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 8 | phase: train | ⏰: 00:19:36\n",
      "Loss: 0.0347 | IoU: 0.2496 | dice: 0.3446 | dice_neg: 1.0000 | dice_pos: 0.3445\n",
      "Starting epoch: 8 | phase: val | ⏰: 01:10:09\n",
      "Loss: 0.0347 | IoU: 0.2355 | dice: 0.3263 | dice_neg: 1.0000 | dice_pos: 0.3262\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 9 | phase: train | ⏰: 01:15:03\n",
      "Loss: 0.0346 | IoU: 0.2507 | dice: 0.3460 | dice_neg: 1.0000 | dice_pos: 0.3458\n",
      "Starting epoch: 9 | phase: val | ⏰: 02:05:48\n",
      "Loss: 0.0346 | IoU: 0.2569 | dice: 0.3534 | dice_neg: 1.0000 | dice_pos: 0.3533\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 10 | phase: train | ⏰: 02:10:42\n",
      "Loss: 0.0329 | IoU: 0.2682 | dice: 0.3665 | dice_neg: 1.0000 | dice_pos: 0.3664\n",
      "Starting epoch: 10 | phase: val | ⏰: 03:01:15\n",
      "Loss: 0.0329 | IoU: 0.2583 | dice: 0.3553 | dice_neg: 1.0000 | dice_pos: 0.3553\n",
      "******** No improvement with patience: 1/4 ********\n",
      "\n",
      "Starting epoch: 11 | phase: train | ⏰: 03:06:08\n",
      "Loss: 0.0323 | IoU: 0.2704 | dice: 0.3695 | dice_neg: 1.0000 | dice_pos: 0.3694\n",
      "Starting epoch: 11 | phase: val | ⏰: 03:56:40\n",
      "Loss: 0.0323 | IoU: 0.2699 | dice: 0.3686 | dice_neg: 1.0000 | dice_pos: 0.3685\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 12 | phase: train | ⏰: 04:01:35\n",
      "Loss: 0.0306 | IoU: 0.2889 | dice: 0.3907 | dice_neg: 1.0000 | dice_pos: 0.3906\n",
      "Starting epoch: 12 | phase: val | ⏰: 04:52:12\n",
      "Loss: 0.0306 | IoU: 0.2807 | dice: 0.3808 | dice_neg: 1.0000 | dice_pos: 0.3807\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 13 | phase: train | ⏰: 04:57:07\n",
      "Loss: 0.0309 | IoU: 0.2851 | dice: 0.3860 | dice_neg: 1.0000 | dice_pos: 0.3858\n",
      "Starting epoch: 13 | phase: val | ⏰: 05:47:37\n",
      "Loss: 0.0309 | IoU: 0.2773 | dice: 0.3770 | dice_neg: 1.0000 | dice_pos: 0.3769\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 14 | phase: train | ⏰: 05:52:35\n",
      "Loss: 0.0312 | IoU: 0.2864 | dice: 0.3881 | dice_neg: 0.0000 | dice_pos: 0.3882\n",
      "Starting epoch: 14 | phase: val | ⏰: 06:43:02\n",
      "Loss: 0.0312 | IoU: 0.2829 | dice: 0.3838 | dice_neg: 0.0000 | dice_pos: 0.3839\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 15 | phase: train | ⏰: 06:47:55\n",
      "Loss: 0.0295 | IoU: 0.3029 | dice: 0.4066 | dice_neg: 0.0000 | dice_pos: 0.4067\n",
      "Starting epoch: 15 | phase: val | ⏰: 07:38:14\n",
      "Loss: 0.0295 | IoU: 0.2911 | dice: 0.3924 | dice_neg: 0.0000 | dice_pos: 0.3924\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 16 | phase: train | ⏰: 07:43:07\n",
      "Loss: 0.0287 | IoU: 0.3087 | dice: 0.4127 | dice_neg: 0.0000 | dice_pos: 0.4128\n",
      "Starting epoch: 16 | phase: val | ⏰: 08:33:35\n",
      "Loss: 0.0287 | IoU: 0.3100 | dice: 0.4144 | dice_neg: 0.0000 | dice_pos: 0.4144\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 17 | phase: train | ⏰: 08:38:28\n",
      "Loss: 0.0282 | IoU: 0.3107 | dice: 0.4153 | dice_neg: 0.0000 | dice_pos: 0.4154\n",
      "Starting epoch: 17 | phase: val | ⏰: 09:28:50\n",
      "Loss: 0.0282 | IoU: 0.3154 | dice: 0.4210 | dice_neg: 0.0000 | dice_pos: 0.4210\n",
      "******** New optimal found, saving state ********\n",
      "\n",
      "Starting epoch: 18 | phase: train | ⏰: 09:33:43\n",
      "Loss: 0.0273 | IoU: 0.3197 | dice: 0.4255 | dice_neg: 0.0000 | dice_pos: 0.4256\n",
      "Starting epoch: 18 | phase: val | ⏰: 10:24:06\n",
      "Loss: 0.0273 | IoU: 0.3208 | dice: 0.4270 | dice_neg: 0.0000 | dice_pos: 0.4270\n",
      "******** No improvement with patience: 1/4 ********\n",
      "\n",
      "Starting epoch: 19 | phase: train | ⏰: 10:28:58\n",
      "Loss: 0.0271 | IoU: 0.3205 | dice: 0.4267 | dice_neg: 0.0000 | dice_pos: 0.4268\n",
      "Starting epoch: 19 | phase: val | ⏰: 11:19:12\n",
      "Loss: 0.0271 | IoU: 0.3165 | dice: 0.4218 | dice_neg: 0.0000 | dice_pos: 0.4218\n",
      "******** No improvement with patience: 2/4 ********\n",
      "\n",
      "Starting epoch: 20 | phase: train | ⏰: 11:24:05\n",
      "Loss: 0.0263 | IoU: 0.3351 | dice: 0.4448 | dice_neg: 0.0000 | dice_pos: 0.4449\n",
      "Starting epoch: 20 | phase: val | ⏰: 12:14:36\n",
      "Loss: 0.0263 | IoU: 0.3359 | dice: 0.4460 | dice_neg: 0.0000 | dice_pos: 0.4461\n",
      "******** No improvement with patience: 3/4 ********\n",
      "\n",
      "Starting epoch: 21 | phase: train | ⏰: 12:19:29\n",
      "Loss: 0.0255 | IoU: 0.3455 | dice: 0.4571 | dice_neg: 0.0000 | dice_pos: 0.4572\n",
      "Starting epoch: 21 | phase: val | ⏰: 13:09:57\n",
      "Loss: 0.0255 | IoU: 0.3317 | dice: 0.4408 | dice_neg: 0.0000 | dice_pos: 0.4409\n",
      "******** No improvement with patience: 4/4 ********\n",
      "Early Stopping\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# define hyperparameters\n",
    "num_epochs = 30\n",
    "batch_size = 4\n",
    "accumulation_steps = 32 / batch_size\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "train_losses = []\n",
    "train_iou = []\n",
    "train_dice = []\n",
    "valid_losses = []\n",
    "valid_iou = []\n",
    "valid_dice = []\n",
    "\n",
    "stop_patience = 4\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "min_val_loss = np.Inf\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    log = Logger(\"train\", epoch)\n",
    "    start = time.strftime(\"%H:%M:%S\")\n",
    "    print(f\"Starting epoch: {epoch} | phase: train | ⏰: {start}\")\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_batches = len(train_loader)\n",
    "    optimizer.zero_grad()\n",
    "    for itr, batch in enumerate(train_loader):\n",
    "        images, targets = batch\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "        if (itr + 1 ) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        running_loss += loss.item()\n",
    "        outputs = outputs.detach()\n",
    "        log.update(targets, outputs)\n",
    "    epoch_loss = (running_loss * accumulation_steps) / total_batches\n",
    "    dice, iou = epoch_log(epoch, epoch_loss, log)\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_dice.append(dice)\n",
    "    train_iou.append(iou)\n",
    "    \n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"best_loss\": best_loss,\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    \n",
    "    # Validation\n",
    "    torch.no_grad()\n",
    "    val_log = Logger(\"val\", epoch)\n",
    "    start = time.strftime(\"%H:%M:%S\")\n",
    "    print(f\"Starting epoch: {epoch} | phase: val | ⏰: {start}\")\n",
    "    running_loss = 0.0\n",
    "    total_batches = len(val_loader)\n",
    "    optimizer.zero_grad()\n",
    "    for itr, batch in enumerate(val_loader):\n",
    "        images, targets = batch\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss = loss / accumulation_steps\n",
    "        running_loss += loss.item()\n",
    "        outputs = outputs.detach()\n",
    "        log.update(targets, outputs)\n",
    "    val_loss = (running_loss * accumulation_steps) / total_batches\n",
    "    dice, iou = epoch_log(epoch, epoch_loss, log)\n",
    "    valid_losses.append(val_loss)\n",
    "    valid_dice.append(dice)\n",
    "    valid_iou.append(iou)\n",
    "    \n",
    "#     scheduler.step(val_loss)\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        print(\"******** New optimal found, saving state ********\")\n",
    "        torch.save(state, \"./models/base-model.pth\")\n",
    "        state[\"best_loss\"] = best_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        min_val_loss = val_loss\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(\"******** No improvement with patience: {}/{} ********\".format(epochs_no_improve, stop_patience))\n",
    "        \n",
    "    if epochs_no_improve == stop_patience:\n",
    "        print(\"Early Stopping\")\n",
    "        early_stop = True\n",
    "        torch.save(state, \"./models/base-model.pth\")\n",
    "        break\n",
    " \n",
    "    print()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet(\n",
      "  (encoder): ResNetEncoder(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): UnetDecoder(\n",
      "    (center): Identity()\n",
      "    (blocks): ModuleList(\n",
      "      (0): DecoderBlock(\n",
      "        (conv1): Conv2dReLU(\n",
      "          (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention1): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "        (conv2): Conv2dReLU(\n",
      "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention2): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "      )\n",
      "      (1): DecoderBlock(\n",
      "        (conv1): Conv2dReLU(\n",
      "          (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention1): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "        (conv2): Conv2dReLU(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention2): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): DecoderBlock(\n",
      "        (conv1): Conv2dReLU(\n",
      "          (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention1): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "        (conv2): Conv2dReLU(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention2): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): DecoderBlock(\n",
      "        (conv1): Conv2dReLU(\n",
      "          (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention1): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "        (conv2): Conv2dReLU(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention2): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): DecoderBlock(\n",
      "        (conv1): Conv2dReLU(\n",
      "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention1): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "        (conv2): Conv2dReLU(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (attention2): Attention(\n",
      "          (attention): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (segmentation_head): SegmentationHead(\n",
      "    (0): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Identity()\n",
      "    (2): Activation(\n",
      "      (activation): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.047507763132006824,\n",
       " 0.04511059701030037,\n",
       " 0.044303568342176386,\n",
       " 0.041112968449977166,\n",
       " 0.0385437869650875,\n",
       " 0.04159449860955516,\n",
       " 0.037681647871961134,\n",
       " 0.03539382869065433,\n",
       " 0.03528842368737577,\n",
       " 0.03514161287851542,\n",
       " 0.03526779063308087,\n",
       " 0.03374255122059209,\n",
       " 0.03342576178688623,\n",
       " 0.0333194823358431,\n",
       " 0.03314540648699074,\n",
       " 0.03249297933485694,\n",
       " 0.030542568443064206,\n",
       " 0.030157070533054584,\n",
       " 0.03257794284490411,\n",
       " 0.031795951366301874,\n",
       " 0.030778167730275384,\n",
       " 0.031842756246824463]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09101097954120257,\n",
       " 0.19988052570667536,\n",
       " 0.22550082104115424,\n",
       " 0.2556054148854991,\n",
       " 0.2676964239269755,\n",
       " 0.28853561944589895,\n",
       " 0.31674041612954107,\n",
       " 0.3369380512108265,\n",
       " 0.32626062331855504,\n",
       " 0.35343933165041663,\n",
       " 0.35534717241748026,\n",
       " 0.3686255076737534,\n",
       " 0.3808251593846374,\n",
       " 0.37695454563007474,\n",
       " 0.3838130890114281,\n",
       " 0.39235852733464177,\n",
       " 0.4143753664173222,\n",
       " 0.42095785286722376,\n",
       " 0.4269684587552757,\n",
       " 0.421779712117633,\n",
       " 0.4460040663173385,\n",
       " 0.44079903213397265]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06102528609125538,\n",
       " 0.13615440321076203,\n",
       " 0.15559938926456066,\n",
       " 0.17941115218958847,\n",
       " 0.1884063216804733,\n",
       " 0.20443758133042017,\n",
       " 0.22582543564145113,\n",
       " 0.2423667915343526,\n",
       " 0.2355177015381089,\n",
       " 0.2568795401136395,\n",
       " 0.258266138554064,\n",
       " 0.26994839672878607,\n",
       " 0.28071899576504167,\n",
       " 0.2772707721289887,\n",
       " 0.2828826418781521,\n",
       " 0.29111199575392616,\n",
       " 0.31004365687295865,\n",
       " 0.3154205501993534,\n",
       " 0.3208358630056838,\n",
       " 0.31647019265171517,\n",
       " 0.33593251540422714,\n",
       " 0.33165861999085156]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07150241661822425,\n",
       " 0.04530455335619771,\n",
       " 0.04324502807700223,\n",
       " 0.040882852488104464,\n",
       " 0.03970169169871613,\n",
       " 0.038044158440566744,\n",
       " 0.03685200896703786,\n",
       " 0.035567230765762986,\n",
       " 0.03468476925520345,\n",
       " 0.03461148846469713,\n",
       " 0.03289948402368097,\n",
       " 0.032325752140969506,\n",
       " 0.03064881803346302,\n",
       " 0.03089100713873057,\n",
       " 0.031192895439673504,\n",
       " 0.029493485908572467,\n",
       " 0.028696761093125453,\n",
       " 0.028161130012561472,\n",
       " 0.027349160990446514,\n",
       " 0.027146287700154353,\n",
       " 0.02628981644624463,\n",
       " 0.025465117150181255]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06964268260120811,\n",
       " 0.1880479630143968,\n",
       " 0.2295215883880224,\n",
       " 0.25877302967647214,\n",
       " 0.2607196666150078,\n",
       " 0.29615744127533583,\n",
       " 0.3118908786151921,\n",
       " 0.33340110724247973,\n",
       " 0.34461766463116306,\n",
       " 0.3459664844497932,\n",
       " 0.36654389473659493,\n",
       " 0.369510849049049,\n",
       " 0.39071300662267117,\n",
       " 0.3859582741417155,\n",
       " 0.38813864291055084,\n",
       " 0.4065985388015799,\n",
       " 0.4127190244800668,\n",
       " 0.41529690606393993,\n",
       " 0.4254964952269529,\n",
       " 0.4266719293075037,\n",
       " 0.4447694511930573,\n",
       " 0.4570935470813632]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04656693519237674,\n",
       " 0.12768885173683056,\n",
       " 0.1593569785621481,\n",
       " 0.1821363143946487,\n",
       " 0.1831146259456904,\n",
       " 0.20991414677854967,\n",
       " 0.22240878171802494,\n",
       " 0.23950430381250423,\n",
       " 0.24955058115312648,\n",
       " 0.2506963054794647,\n",
       " 0.26819265257144637,\n",
       " 0.2704295176346331,\n",
       " 0.28894094244982504,\n",
       " 0.28511459079610496,\n",
       " 0.2864017026798611,\n",
       " 0.3029288965489734,\n",
       " 0.3086857212945411,\n",
       " 0.3106605263495961,\n",
       " 0.31965546337508727,\n",
       " 0.3204903980015809,\n",
       " 0.3351293910839219,\n",
       " 0.34545985060721346]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iou)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
